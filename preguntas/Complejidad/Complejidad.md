# Preguntas sobre Complejidad Algor√≠tmica (Big O)

---

## 1. ¬øQu√© significa que un algoritmo tenga complejidad O(n)?

Un algoritmo O(n) significa que su tiempo de ejecuci√≥n crece linealmente con respecto al tama√±o de la entrada.

- Si la entrada tiene 10 elementos, har√° aproximadamente 10 operaciones.
- Si tiene 1.000 elementos, har√° 1.000 operaciones.

### Ejemplo:
```python
def buscar_elemento(lista, objetivo):
    for x in lista:
        if x == objetivo:
            return True
    return False
```

 ## 2. ¬øCu√°l es la complejidad de buscar un elemento en una lista vs. en un diccionario?

| Estructura  | Complejidad de b√∫squeda | Notaci√≥n |
|-------------|--------------------------|----------|
| Lista       | Lineal                   | O(n)     |
| Diccionario | Constante promedio       | O(1)     |

**Nota:** En casos extremos de colisiones, un diccionario puede degradarse a `O(n)`, pero eso es raro con un buen sistema de *hashing*.

## 3. ¬øQu√© complejidad tiene recorrer una lista de listas (matriz) de n √ó m elementos?

La complejidad es `O(n √ó m)`, porque hay que iterar sobre cada fila y cada columna dentro de cada fila.

üß™ **Ejemplo:**

```python
def sumar_matriz(matriz):
    suma = 0
    for fila in matriz:
        for valor in fila:
            suma += valor
    return suma
```

## 4. ¬øCu√°l es la complejidad del siguiente fragmento de c√≥digo?
```python
for i in range(n):
    for j in range(n):
        print(i, j)
```
Cada iteraci√≥n externa se repite n veces, y dentro de ella hay otras n repeticiones ‚Üí n √ó n.

## Pregunta 5: ¬øQu√© complejidad tiene una b√∫squeda binaria en una lista ordenada?
O(log n)
Porque en cada paso se reduce el tama√±o del problema a la mitad.

```python
def busqueda_binaria(lista, objetivo):
    izquierda, derecha = 0, len(lista) - 1
    while izquierda <= derecha:
        medio = (izquierda + derecha) // 2
        if lista[medio] == objetivo:
            return medio
        elif lista[medio] < objetivo:
            izquierda = medio + 1
        else:
            derecha = medio - 1
    return -1
```


## 6. ¬øCu√°l es el impacto de la recursividad en la complejidad temporal?

- La recursividad puede ser poderosa, pero tambi√©n costosa si no se optimiza.
- Muchas funciones recursivas tienen complejidad exponencial si recalculan subproblemas.

### Ejemplo cl√°sico: Fibonacci

```python
def fib(n):
    if n <= 1:
        return n
    return fib(n - 1) + fib(n - 2)
```

Complejidad: O(2‚Åø) ‚Üí ineficiente para valores grandes.

Soluci√≥n: memoizaci√≥n o programaci√≥n din√°mica
```python
def fib_dp(n):
    memo = [0, 1]
    for i in range(2, n+1):
        memo.append(memo[i-1] + memo[i-2])
    return memo[n]
```

Complejidad: O(n)


## 7. ¬øCu√°ndo conviene usar una tabla hash (`dict`) en lugar de una lista?

Usa un **diccionario (tabla hash)** cuando:

- Necesit√°s b√∫squedas r√°pidas por clave ‚Üí O(1) promedio.
- Hay necesidad de acceder a elementos por identificador.

Usa una **lista** cuando:

- El orden importa o es secuencial.
- Se necesita recorrer todos los elementos en orden.

### Ejemplo:

```python
# B√∫squeda en lista (O(n))
for producto in productos:
    if producto.id == 5:
        return producto

# B√∫squeda en dict (O(1))
productos_por_id = {p.id: p for p in productos}
return productos_por_id[5]
```


## 8. ¬øQu√© complejidad tienen las funciones `map`, `filter` y `reduce` en listas?

Todas estas funciones recorren la lista **una sola vez**, por lo que tienen complejidad **O(n)**.

- `map`: aplica una funci√≥n a cada elemento ‚Üí O(n)
- `filter`: eval√∫a una condici√≥n sobre cada elemento ‚Üí O(n)
- `reduce`: combina elementos usando una operaci√≥n acumulativa ‚Üí O(n)

### Ejemplo en Python:
```python
nombres = ["Ana", "Luis", "Carlos"]

# map
mayusculas = list(map(str.upper, nombres))

# filter
largos = list(filter(lambda x: len(x) > 3, nombres))

# reduce
from functools import reduce
total_letras = reduce(lambda acc, x: acc + len(x), nombres, 0)
```

Nota:
Aunque O(n), estas funciones son muy eficientes porque est√°n optimizadas internamente.


## 9. ¬øQu√© prefieres entre un algoritmo O(n log n) y uno O(n¬≤), y por qu√©?

En general, un algoritmo **O(n log n)** es m√°s eficiente que uno **O(n¬≤)**, especialmente cuando `n` (el tama√±o de la entrada) es grande.

### Comparaci√≥n pr√°ctica:

| n     | n log n (base 2) | n¬≤     |
|-------|------------------|--------|
| 10    | ~33              | 100    |
| 100   | ~664             | 10,000 |
| 1,000 | ~9,966           | 1,000,000 |

‚Üí A medida que `n` crece, la diferencia se vuelve enorme.

### Ejemplo real:
- O(n¬≤): burbuja, selecci√≥n, inserci√≥n (ordenamiento lentos).
- O(n log n): mergesort, heapsort (ordenamiento eficientes).

> Incluso si O(n¬≤) puede ser m√°s simple de implementar, **en producci√≥n se prioriza la eficiencia y escalabilidad**.

Siempre que sea posible, **prefiere algoritmos con mejor complejidad asint√≥tica**, especialmente en sistemas con muchos datos o usuarios concurrentes.

## 10. ¬øQu√© representa la notaci√≥n Big O y por qu√© es importante en programaci√≥n backend?

La **notaci√≥n Big O** describe el **crecimiento del tiempo o espacio** que requiere un algoritmo en funci√≥n del tama√±o de entrada `n`. Es una forma de analizar la eficiencia algor√≠tmica.

### ¬øPor qu√© es importante?
- Permite comparar algoritmos de forma te√≥rica.
- Ayuda a anticipar problemas de rendimiento en sistemas con muchos usuarios o datos.
- Es clave para tomar decisiones t√©cnicas informadas.

### Ejemplos comunes:

| Complejidad | Nombre           | Ejemplo                          |
|-------------|------------------|----------------------------------|
| O(1)        | Constante        | Acceso a un elemento en un array |
| O(log n)    | Logar√≠tmica      | B√∫squeda binaria                 |
| O(n)        | Lineal           | Recorrer un arreglo              |
| O(n log n)  | Lineal logar√≠tmica | Mergesort, Quicksort (mejor caso) |
| O(n¬≤)       | Cuadr√°tica       | Burbujas, selecci√≥n, inserci√≥n   |

### Ejemplo en c√≥digo:

```python
# O(n): recorre una lista
def suma(lista):
    total = 0
    for num in lista:
        total += num
    return total
```

### Consideraci√≥n:

Big O no mide tiempos reales, sino crecimiento relativo. Depende del peor caso (salvo que se aclare lo contrario).

> Dominar Big O permite escribir c√≥digo m√°s escalable y tomar mejores decisiones en el dise√±o backend.

## 11. ¬øCu√°l es la complejidad del siguiente c√≥digo y por qu√©?

```python
def buscar_pares(lista):
    for i in lista:
        for j in lista:
            if (i + j) % 2 == 0:
                print(i, j)
```

El c√≥digo tiene dos bucles anidados que recorren la lista completa.

Si la lista tiene n elementos, el n√∫mero total de combinaciones es n * n ‚Üí O(n¬≤).


### An√°lisis:

| L√≠nea                 | Operaci√≥n                         | Complejidad |
|----------------------|-----------------------------------|-------------|
| `for i in lista:`    | Recorre `n` elementos             | O(n)        |
| `for j in lista:`    | Anidado, se ejecuta por cada `i`  | O(n)        |
| `print(i, j)`        | Operaci√≥n constante               | O(1)        |


‚Üí Complejidad total: O(n¬≤)

### ¬øC√≥mo podr√≠a optimizarse?

Si no necesitas combinaciones repetidas ((i,j) y (j,i)), puedes usar for j in lista[i+1:] para reducir a aproximadamente la mitad las iteraciones.

Pero la complejidad seguir√≠a siendo O(n¬≤), solo con un mejor coeficiente.


> Saber leer y analizar c√≥digo te permite anticipar cuellos de botella en tiempo de ejecuci√≥n.

## 12. ¬øCu√°l es la complejidad de las operaciones b√°sicas en un diccionario (hashmap) y por qu√©?

Los **diccionarios** (tambi√©n llamados **hashmaps** o **tablas hash**) permiten almacenar y acceder a datos por clave. En la mayor√≠a de los lenguajes modernos (como Python, JavaScript, Java), est√°n implementados con **hash tables**.

### Complejidades comunes:

| Operaci√≥n         | Complejidad Promedio | Complejidad Peor Caso |
|-------------------|----------------------|------------------------|
| Insertar (`put`)  | O(1)                 | O(n) (en caso de colisiones extremas) |
| Acceder (`get`)   | O(1)                 | O(n)                   |
| Eliminar (`del`)  | O(1)                 | O(n)                   |

> En la pr√°ctica, con buenas funciones de hash y mecanismos de resoluci√≥n de colisiones, el rendimiento se mantiene cercano a **O(1)**.

### Ejemplo en Python:

```python
usuarios = {"ana": 25, "juan": 30}
usuarios["ana"]  # acceso en O(1)
usuarios["carlos"] = 22  # inserci√≥n en O(1)
```

### Consideraciones:

La eficiencia depende de la funci√≥n hash y la implementaci√≥n interna.

En situaciones donde hay muchas colisiones, el rendimiento puede degradarse.


> Los diccionarios son una de las estructuras m√°s eficientes para b√∫squedas, pero no siempre son la mejor opci√≥n si se necesita orden o evitar colisiones.

## 13 ¬øQu√© es la complejidad espacial y c√≥mo se diferencia de la complejidad temporal?

La **complejidad espacial** mide cu√°nta **memoria adicional** necesita un algoritmo para ejecutarse, en funci√≥n del tama√±o de la entrada.

### Comparaci√≥n:

| Tipo de complejidad | Qu√© mide                    | Ejemplo de unidad         |
|---------------------|-----------------------------|---------------------------|
| Temporal            | Tiempo de ejecuci√≥n         | Operaciones / pasos       |
| Espacial            | Memoria utilizada           | Variables, arreglos, etc. |

### Ejemplo simple:

```python
def suma(lista):
    total = 0      # O(1)
    for num in lista:
        total += num
    return total
```

Complejidad temporal: O(n) (por el bucle)

Complejidad espacial: O(1) (solo una variable total)


### Ejemplo con mayor uso de memoria:

```python
def duplicar(lista):
    nueva = []
    for x in lista:
        nueva.append(x * 2)
    return nueva
```

Aqu√≠, la complejidad espacial es O(n), porque se crea una nueva lista del mismo tama√±o que la original.


### ¬øPor qu√© es importante?

En sistemas con memoria limitada (embedded, m√≥viles).

Cuando se procesan grandes vol√∫menes de datos.

Para optimizar algoritmos m√°s all√° del tiempo.


> Una soluci√≥n r√°pida que consume demasiada memoria puede no ser viable en producci√≥n. El equilibrio entre tiempo y espacio es clave.

## 14. ¬øCu√°l es la complejidad temporal de los algoritmos de ordenamiento m√°s comunes?

A continuaci√≥n se muestra una tabla con la complejidad temporal de algunos algoritmos de ordenamiento conocidos:

| Algoritmo          | Mejor caso       | Promedio         | Peor caso          | Estable |
|--------------------|------------------|------------------|--------------------|---------|
| Bubble Sort        | O(n)             | O(n¬≤)            | O(n¬≤)              | S√≠      |
| Insertion Sort     | O(n)             | O(n¬≤)            | O(n¬≤)              | S√≠      |
| Selection Sort     | O(n¬≤)            | O(n¬≤)            | O(n¬≤)              | No      |
| Merge Sort         | O(n log n)       | O(n log n)       | O(n log n)         | S√≠      |
| Quick Sort         | O(n log n)       | O(n log n)       | O(n¬≤)              | No      |
| Heap Sort          | O(n log n)       | O(n log n)       | O(n log n)         | No      |

### Tip:
- **Estabilidad** significa que los elementos con valores iguales mantienen su orden relativo original.
- QuickSort es muy r√°pido en la pr√°ctica, pero su peor caso puede ser problem√°tico si no se elige bien el pivote.
- MergeSort es preferido cuando se necesita un algoritmo estable con buen rendimiento garantizado.

## 15. ¬øCu√°l es la complejidad temporal de acceso, inserci√≥n y eliminaci√≥n en distintas estructuras de datos?


| Estructura de datos      | Acceso     | Inserci√≥n | Eliminaci√≥n |
|--------------------------|------------|-----------|-------------|
| Array (est√°tico)         | O(1)       | O(n)      | O(n)        |
| Lista enlazada simple    | O(n)       | O(1)      | O(1) (al inicio) |
| Pila (stack)             | O(n)       | O(1)      | O(1)        |
| Cola (queue)             | O(n)       | O(1)      | O(1)        |
| Hash Table               | O(1)       | O(1)      | O(1)        |
| √Årbol binario balanceado (AVL, Red-Black) | O(log n) | O(log n) | O(log n) |

### Tip:
- El array permite acceso directo (√≠ndice), pero no es eficiente para insertar o borrar elementos.
- Las hash tables ofrecen acceso e inserci√≥n promedio en tiempo constante, pero pueden degradarse a O(n) en caso de muchas colisiones.
- √Årboles balanceados garantizan operaciones eficientes incluso en el peor caso.

## 16. ¬øCu√°l es la complejidad temporal del siguiente algoritmo recursivo?

```python
def f(n):
    if n <= 1:
        return 1
    return f(n - 1) + f(n - 1)
```

Este algoritmo llama a f(n-1) dos veces en cada nivel de recursi√≥n. Podemos expresar su complejidad como:

T(n) = 2 * T(n - 1) + O(1)

La soluci√≥n a esta recurrencia es:

T(n) = O(2^n)

### Explicaci√≥n:

En cada nivel, el n√∫mero de llamadas se duplica.

La altura del √°rbol de recursi√≥n es n, y hay 2^n llamadas al final.


Esta es una recursi√≥n exponencial, muy costosa en tiempo.

> Optimizaci√≥n posible: usar memoization o programaci√≥n din√°mica para reducir a O(n).