# Pr..eguntas sobre Complejidad Algor√≠tmica (Big O)

---

## 1. ¬øQu√© significa que un algoritmo tenga complejidad O(n)?

Un algoritmo O(n) significa que su tiempo de ejecuci√≥n crece linealmente con respecto al tama√±o de la entrada.

- Si la entrada tiene 10 elementos, har√° aproximadamente 10 operaciones.
- Si tiene 1.000 elementos, har√° 1.000 operaciones.

### Ejemplo:
```python
def buscar_elemento(lista, objetivo):
    for x in lista:
        if x == objetivo:
            return True
    return False
```

 ## 2. ¬øCu√°l es la complejidad de buscar un elemento en una lista vs. en un diccionario?

| Estructura  | Complejidad de b√∫squeda | Notaci√≥n |
|-------------|--------------------------|----------|
| Lista       | Lineal                   | O(n)     |
| Diccionario | Constante promedio       | O(1)     |

**Nota:** En casos extremos de colisiones, un diccionario puede degradarse a `O(n)`, pero eso es raro con un buen sistema de *hashing*.

## 3. ¬øQu√© complejidad tiene recorrer una lista de listas (matriz) de n √ó m elementos?

La complejidad es `O(n √ó m)`, porque hay que iterar sobre cada fila y cada columna dentro de cada fila.

üß™ **Ejemplo:**

```python
def sumar_matriz(matriz):
    suma = 0
    for fila in matriz:
        for valor in fila:
            suma += valor
    return suma
```

## 4. ¬øCu√°l es la complejidad del siguiente fragmento de c√≥digo?
```python
for i in range(n):
    for j in range(n):
        print(i, j)
```
Cada iteraci√≥n externa se repite n veces, y dentro de ella hay otras n repeticiones ‚Üí n √ó n.

## Pregunta 5: ¬øQu√© complejidad tiene una b√∫squeda binaria en una lista ordenada?
O(log n)
Porque en cada paso se reduce el tama√±o del problema a la mitad.

```python
def busqueda_binaria(lista, objetivo):
    izquierda, derecha = 0, len(lista) - 1
    while izquierda <= derecha:
        medio = (izquierda + derecha) // 2
        if lista[medio] == objetivo:
            return medio
        elif lista[medio] < objetivo:
            izquierda = medio + 1
        else:
            derecha = medio - 1
    return -1
```


## 6. ¬øCu√°l es el impacto de la recursividad en la complejidad temporal?

- La recursividad puede ser poderosa, pero tambi√©n costosa si no se optimiza.
- Muchas funciones recursivas tienen complejidad exponencial si recalculan subproblemas.

### Ejemplo cl√°sico: Fibonacci

```python
def fib(n):
    if n <= 1:
        return n
    return fib(n - 1) + fib(n - 2)
```

Complejidad: O(2‚Åø) ‚Üí ineficiente para valores grandes.

Soluci√≥n: memoizaci√≥n o programaci√≥n din√°mica
```python
def fib_dp(n):
    memo = [0, 1]
    for i in range(2, n+1):
        memo.append(memo[i-1] + memo[i-2])
    return memo[n]
```

Complejidad: O(n)


## 7. ¬øCu√°ndo conviene usar una tabla hash (`dict`) en lugar de una lista?

Usa un **diccionario (tabla hash)** cuando:

- Necesit√°s b√∫squedas r√°pidas por clave ‚Üí O(1) promedio.
- Hay necesidad de acceder a elementos por identificador.

Usa una **lista** cuando:

- El orden importa o es secuencial.
- Se necesita recorrer todos los elementos en orden.

### Ejemplo:

```python
# B√∫squeda en lista (O(n))
for producto in productos:
    if producto.id == 5:
        return producto

# B√∫squeda en dict (O(1))
productos_por_id = {p.id: p for p in productos}
return productos_por_id[5]
```


## 8. ¬øQu√© complejidad tienen las funciones `map`, `filter` y `reduce` en listas?

Todas estas funciones recorren la lista **una sola vez**, por lo que tienen complejidad **O(n)**.

- `map`: aplica una funci√≥n a cada elemento ‚Üí O(n)
- `filter`: eval√∫a una condici√≥n sobre cada elemento ‚Üí O(n)
- `reduce`: combina elementos usando una operaci√≥n acumulativa ‚Üí O(n)

### Ejemplo en Python:
```python
nombres = ["Ana", "Luis", "Carlos"]

# map
mayusculas = list(map(str.upper, nombres))

# filter
largos = list(filter(lambda x: len(x) > 3, nombres))

# reduce
from functools import reduce
total_letras = reduce(lambda acc, x: acc + len(x), nombres, 0)
```

Nota:
Aunque O(n), estas funciones son muy eficientes porque est√°n optimizadas internamente.


## 9. ¬øQu√© prefieres entre un algoritmo O(n log n) y uno O(n¬≤), y por qu√©?

En general, un algoritmo **O(n log n)** es m√°s eficiente que uno **O(n¬≤)**, especialmente cuando `n` (el tama√±o de la entrada) es grande.

### Comparaci√≥n pr√°ctica:

| n     | n log n (base 2) | n¬≤     |
|-------|------------------|--------|
| 10    | ~33              | 100    |
| 100   | ~664             | 10,000 |
| 1,000 | ~9,966           | 1,000,000 |

‚Üí A medida que `n` crece, la diferencia se vuelve enorme.

### Ejemplo real:
- O(n¬≤): burbuja, selecci√≥n, inserci√≥n (ordenamiento lentos).
- O(n log n): mergesort, heapsort (ordenamiento eficientes).

> Incluso si O(n¬≤) puede ser m√°s simple de implementar, **en producci√≥n se prioriza la eficiencia y escalabilidad**.

Siempre que sea posible, **prefiere algoritmos con mejor complejidad asint√≥tica**, especialmente en sistemas con muchos datos o usuarios concurrentes.

## 10. ¬øQu√© representa la notaci√≥n Big O y por qu√© es importante en programaci√≥n backend?

La **notaci√≥n Big O** describe el **crecimiento del tiempo o espacio** que requiere un algoritmo en funci√≥n del tama√±o de entrada `n`. Es una forma de analizar la eficiencia algor√≠tmica.

### ¬øPor qu√© es importante?
- Permite comparar algoritmos de forma te√≥rica.
- Ayuda a anticipar problemas de rendimiento en sistemas con muchos usuarios o datos.
- Es clave para tomar decisiones t√©cnicas informadas.

### Ejemplos comunes:

| Complejidad | Nombre           | Ejemplo                          |
|-------------|------------------|----------------------------------|
| O(1)        | Constante        | Acceso a un elemento en un array |
| O(log n)    | Logar√≠tmica      | B√∫squeda binaria                 |
| O(n)        | Lineal           | Recorrer un arreglo              |
| O(n log n)  | Lineal logar√≠tmica | Mergesort, Quicksort (mejor caso) |
| O(n¬≤)       | Cuadr√°tica       | Burbujas, selecci√≥n, inserci√≥n   |

### Ejemplo en c√≥digo:

```python
# O(n): recorre una lista
def suma(lista):
    total = 0
    for num in lista:
        total += num
    return total
```

### Consideraci√≥n:

Big O no mide tiempos reales, sino crecimiento relativo. Depende del peor caso (salvo que se aclare lo contrario).

> Dominar Big O permite escribir c√≥digo m√°s escalable y tomar mejores decisiones en el dise√±o backend.

## 11. ¬øCu√°l es la complejidad del siguiente c√≥digo y por qu√©?

```python
def buscar_pares(lista):
    for i in lista:
        for j in lista:
            if (i + j) % 2 == 0:
                print(i, j)
```

El c√≥digo tiene dos bucles anidados que recorren la lista completa.

Si la lista tiene n elementos, el n√∫mero total de combinaciones es n * n ‚Üí O(n¬≤).


### An√°lisis:

| L√≠nea                 | Operaci√≥n                         | Complejidad |
|----------------------|-----------------------------------|-------------|
| `for i in lista:`    | Recorre `n` elementos             | O(n)        |
| `for j in lista:`    | Anidado, se ejecuta por cada `i`  | O(n)        |
| `print(i, j)`        | Operaci√≥n constante               | O(1)        |


‚Üí Complejidad total: O(n¬≤)

### ¬øC√≥mo podr√≠a optimizarse?

Si no necesitas combinaciones repetidas ((i,j) y (j,i)), puedes usar for j in lista[i+1:] para reducir a aproximadamente la mitad las iteraciones.

Pero la complejidad seguir√≠a siendo O(n¬≤), solo con un mejor coeficiente.


> Saber leer y analizar c√≥digo te permite anticipar cuellos de botella en tiempo de ejecuci√≥n.

## 12. ¬øCu√°l es la complejidad de las operaciones b√°sicas en un diccionario (hashmap) y por qu√©?

Los **diccionarios** (tambi√©n llamados **hashmaps** o **tablas hash**) permiten almacenar y acceder a datos por clave. En la mayor√≠a de los lenguajes modernos (como Python, JavaScript, Java), est√°n implementados con **hash tables**.

### Complejidades comunes:

| Operaci√≥n         | Complejidad Promedio | Complejidad Peor Caso |
|-------------------|----------------------|------------------------|
| Insertar (`put`)  | O(1)                 | O(n) (en caso de colisiones extremas) |
| Acceder (`get`)   | O(1)                 | O(n)                   |
| Eliminar (`del`)  | O(1)                 | O(n)                   |

> En la pr√°ctica, con buenas funciones de hash y mecanismos de resoluci√≥n de colisiones, el rendimiento se mantiene cercano a **O(1)**.

### Ejemplo en Python:

```python
usuarios = {"ana": 25, "juan": 30}
usuarios["ana"]  # acceso en O(1)
usuarios["carlos"] = 22  # inserci√≥n en O(1)
```

### Consideraciones:

La eficiencia depende de la funci√≥n hash y la implementaci√≥n interna.

En situaciones donde hay muchas colisiones, el rendimiento puede degradarse.


> Los diccionarios son una de las estructuras m√°s eficientes para b√∫squedas, pero no siempre son la mejor opci√≥n si se necesita orden o evitar colisiones.

## 13 ¬øQu√© es la complejidad espacial y c√≥mo se diferencia de la complejidad temporal?

La **complejidad espacial** mide cu√°nta **memoria adicional** necesita un algoritmo para ejecutarse, en funci√≥n del tama√±o de la entrada.

### Comparaci√≥n:

| Tipo de complejidad | Qu√© mide                    | Ejemplo de unidad         |
|---------------------|-----------------------------|---------------------------|
| Temporal            | Tiempo de ejecuci√≥n         | Operaciones / pasos       |
| Espacial            | Memoria utilizada           | Variables, arreglos, etc. |

### Ejemplo simple:

```python
def suma(lista):
    total = 0      # O(1)
    for num in lista:
        total += num
    return total
```

Complejidad temporal: O(n) (por el bucle)

Complejidad espacial: O(1) (solo una variable total)


### Ejemplo con mayor uso de memoria:

```python
def duplicar(lista):
    nueva = []
    for x in lista:
        nueva.append(x * 2)
    return nueva
```

Aqu√≠, la complejidad espacial es O(n), porque se crea una nueva lista del mismo tama√±o que la original.


### ¬øPor qu√© es importante?

En sistemas con memoria limitada (embedded, m√≥viles).

Cuando se procesan grandes vol√∫menes de datos.

Para optimizar algoritmos m√°s all√° del tiempo.


> Una soluci√≥n r√°pida que consume demasiada memoria puede no ser viable en producci√≥n. El equilibrio entre tiempo y espacio es clave.

## 14. ¬øCu√°l es la complejidad temporal de los algoritmos de ordenamiento m√°s comunes?

A continuaci√≥n se muestra una tabla con la complejidad temporal de algunos algoritmos de ordenamiento conocidos:

| Algoritmo          | Mejor caso       | Promedio         | Peor caso          | Estable |
|--------------------|------------------|------------------|--------------------|---------|
| Bubble Sort        | O(n)             | O(n¬≤)            | O(n¬≤)              | S√≠      |
| Insertion Sort     | O(n)             | O(n¬≤)            | O(n¬≤)              | S√≠      |
| Selection Sort     | O(n¬≤)            | O(n¬≤)            | O(n¬≤)              | No      |
| Merge Sort         | O(n log n)       | O(n log n)       | O(n log n)         | S√≠      |
| Quick Sort         | O(n log n)       | O(n log n)       | O(n¬≤)              | No      |
| Heap Sort          | O(n log n)       | O(n log n)       | O(n log n)         | No      |

### Tip:
- **Estabilidad** significa que los elementos con valores iguales mantienen su orden relativo original.
- QuickSort es muy r√°pido en la pr√°ctica, pero su peor caso puede ser problem√°tico si no se elige bien el pivote.
- MergeSort es preferido cuando se necesita un algoritmo estable con buen rendimiento garantizado.

## 15. ¬øCu√°l es la complejidad temporal de acceso, inserci√≥n y eliminaci√≥n en distintas estructuras de datos?


| Estructura de datos      | Acceso     | Inserci√≥n | Eliminaci√≥n |
|--------------------------|------------|-----------|-------------|
| Array (est√°tico)         | O(1)       | O(n)      | O(n)        |
| Lista enlazada simple    | O(n)       | O(1)      | O(1) (al inicio) |
| Pila (stack)             | O(n)       | O(1)      | O(1)        |
| Cola (queue)             | O(n)       | O(1)      | O(1)        |
| Hash Table               | O(1)       | O(1)      | O(1)        |
| √Årbol binario balanceado (AVL, Red-Black) | O(log n) | O(log n) | O(log n) |

### Tip:
- El array permite acceso directo (√≠ndice), pero no es eficiente para insertar o borrar elementos.
- Las hash tables ofrecen acceso e inserci√≥n promedio en tiempo constante, pero pueden degradarse a O(n) en caso de muchas colisiones.
- √Årboles balanceados garantizan operaciones eficientes incluso en el peor caso.

## 16. ¬øCu√°l es la complejidad temporal del siguiente algoritmo recursivo?

```python
def f(n):
    if n <= 1:
        return 1
    return f(n - 1) + f(n - 1)
```

Este algoritmo llama a f(n-1) dos veces en cada nivel de recursi√≥n. Podemos expresar su complejidad como:

T(n) = 2 * T(n - 1) + O(1)

La soluci√≥n a esta recurrencia es:

T(n) = O(2^n)

### Explicaci√≥n:

En cada nivel, el n√∫mero de llamadas se duplica.

La altura del √°rbol de recursi√≥n es n, y hay 2^n llamadas al final.


Esta es una recursi√≥n exponencial, muy costosa en tiempo.

> Optimizaci√≥n posible: usar memoization o programaci√≥n din√°mica para reducir a O(n).


## 17. Dado un array de enteros, ¬øc√≥mo encontrar la subcadena contigua con la suma m√°s alta en el menor tiempo posible?

Ejemplo:
```python
nums = [-2, 1, -3, 4, -1, 2, 1, -5, 4]
```

Este es un problema cl√°sico de optimizaci√≥n conocido como el m√°ximo subarreglo contiguo (Maximum Subarray Problem), resuelto eficientemente con el algoritmo de Kadane.

### Enfoque:

def max_subarray(nums):
    max_actual = max_total = nums[0]
    for n in nums[1:]:
        max_actual = max(n, max_actual + n)
        max_total = max(max_total, max_actual)
    return max_total

### Complejidad:

Tiempo: O(n)

Espacio: O(1)


### Explicaci√≥n:

Se recorre el array una sola vez.

Se acumula la suma m√°xima posible terminando en cada posici√≥n.

Si una suma acumulada es menor que el n√∫mero actual, se reinicia desde all√≠.

Este algoritmo evita c√°lculos innecesarios como en la soluci√≥n de fuerza bruta O(n¬≤).

## 18. ¬øQu√© significa que un algoritmo tenga complejidad **tiempo lineal** pero **espacio logar√≠tmico**?

Significa que:

- El **tiempo de ejecuci√≥n** del algoritmo crece proporcionalmente al tama√±o de la entrada (O(n)).
- El **uso de memoria adicional** (espacio auxiliar) crece en proporci√≥n al logaritmo del tama√±o de la entrada (O(log n)).

### Ejemplo:

```python
def recursivo_log(n):
    if n <= 1:
        return
    recursivo_log(n // 2)

def linear_scan(arr):
    for x in arr:
        print(x)
```

linear_scan tiene tiempo O(n) y espacio O(1).

recursivo_log tiene tiempo O(log n) y usa O(log n) espacio por la pila de llamadas recursivas.


Si combinamos un recorrido lineal con llamadas recursivas logar√≠tmicas, podr√≠amos tener un algoritmo con tiempo O(n) y espacio O(log n).


> Una buena comprensi√≥n de c√≥mo var√≠an tiempo y espacio seg√∫n la estrategia (recursi√≥n, iteraci√≥n, estructuras auxiliares) es clave para optimizar software backend eficiente.


## Analiza la complejidad temporal y espacial del siguiente algoritmo, e indica si puede optimizarse: 

```python
def buscar_suma(lista, objetivo):
    for i in range(len(lista)):
        for j in range(len(lista)):
            if lista[i] + lista[j] == objetivo:
                return True
    return False
```

### 1. Complejidad temporal

El algoritmo usa dos bucles anidados de tama√±o n, por lo que:

Tiempo: O(n¬≤) en el peor de los casos.


### 2. Complejidad espacial

Usa solo variables auxiliares (i, j), por lo que:

Espacio: O(1).


### 3. Posible optimizaci√≥n

Usar una estructura de datos como set para almacenar los elementos ya vistos y verificar complementos en tiempo constante:

Nuevo tiempo: O(n)

Nuevo espacio: O(n)

## 19. Analiza la complejidad temporal y espacial del algoritmo de b√∫squeda binaria y explica en qu√© condiciones es m√°s eficiente que la b√∫squeda lineal.

1. **Complejidad temporal:**
   - B√∫squeda binaria: O(log n) en el mejor, promedio y peor caso.
   - B√∫squeda lineal: O(n) en el peor caso.

2. **Complejidad espacial:**
   - B√∫squeda binaria: O(1) si es iterativa, O(log n) si es recursiva (por el stack).
   - B√∫squeda lineal: O(1).

3. **Condiciones de eficiencia:**
   - La b√∫squeda binaria requiere que la colecci√≥n est√© ordenada.
   - Es m√°s eficiente que la b√∫squeda lineal cuando el tama√±o de la colecci√≥n es grande y ya est√° ordenada.

4. **Ejemplo de comparaci√≥n:**
   - En una lista ordenada de un mill√≥n de elementos:
     - B√∫squeda lineal ‚Üí hasta 1,000,000 comparaciones.
     - B√∫squeda binaria ‚Üí m√°ximo ~20 comparaciones.

## 20. Determina la complejidad temporal y espacial de un algoritmo de ordenamiento por burbuja (Bubble Sort) y comp√°ralo con el de QuickSort. Explica en qu√© escenarios podr√≠a justificarse el uso de Bubble Sort.

1. **Complejidad temporal:**
   - **Bubble Sort:**
     - Mejor caso: O(n) (cuando la lista ya est√° ordenada y se optimiza el algoritmo).
     - Promedio y peor caso: O(n¬≤).
   - **QuickSort:**
     - Mejor y promedio: O(n log n).
     - Peor caso: O(n¬≤) (cuando siempre se elige un pivote extremo en listas ya ordenadas).

2. **Complejidad espacial:**
   - **Bubble Sort:** O(1) (in-place).
   - **QuickSort:** O(log n) por la pila de recursi√≥n.

3. **Escenarios donde Bubble Sort puede ser √∫til:**
   - Listas muy peque√±as.
   - Cuando la lista ya est√° casi ordenada y se usa la versi√≥n optimizada.
   - En entornos educativos para explicar conceptos b√°sicos de ordenamiento.

4. **Ejemplo comparativo:**
   - Lista de 10,000 elementos aleatorios:
     - Bubble Sort ‚Üí ~100,000,000 comparaciones.
     - QuickSort ‚Üí ~132,877 comparaciones en promedio.

> Aunque Bubble Sort es ineficiente para grandes vol√∫menes de datos, su simplicidad y facilidad de implementaci√≥n lo hacen √∫til en casos muy espec√≠ficos.